{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0162905e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jr\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import desisky\n",
    "from desisky.io import load_builtin\n",
    "from desisky.data import SkySpecVAC\n",
    "from desisky.training import NumpyLoader\n",
    "from desisky.models.vae import make_SkyVAE\n",
    "import equinox as eqx\n",
    "import optax \n",
    "import torch \n",
    "from torch.utils.data import random_split\n",
    "import os\n",
    "import json \n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e00ea028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAE Architecture:\n",
      "  Input channels: 7781\n",
      "  Latent dimension: 8\n",
      "\n",
      "Model schema version: 0\n",
      "SkyVAE(\n",
      "  in_channels=7781,\n",
      "  latent_dim=8,\n",
      "  common_fc=Sequential(\n",
      "    layers=(\n",
      "      Linear(\n",
      "        weight=f32[1000,7781],\n",
      "        bias=f32[1000],\n",
      "        in_features=7781,\n",
      "        out_features=1000,\n",
      "        use_bias=True\n",
      "      ),\n",
      "      Lambda(fn=<PjitFunction of <function relu at 0x7f986cbc23e0>>),\n",
      "      Linear(\n",
      "        weight=f32[800,1000],\n",
      "        bias=f32[800],\n",
      "        in_features=1000,\n",
      "        out_features=800,\n",
      "        use_bias=True\n",
      "      ),\n",
      "      Lambda(fn=<PjitFunction of <function relu at 0x7f986cbc23e0>>),\n",
      "      Linear(\n",
      "        weight=f32[600,800],\n",
      "        bias=f32[600],\n",
      "        in_features=800,\n",
      "        out_features=600,\n",
      "        use_bias=True\n",
      "      ),\n",
      "      Lambda(fn=<PjitFunction of <function relu at 0x7f986cbc23e0>>),\n",
      "      Linear(\n",
      "        weight=f32[500,600],\n",
      "        bias=f32[500],\n",
      "        in_features=600,\n",
      "        out_features=500,\n",
      "        use_bias=True\n",
      "      ),\n",
      "      Lambda(fn=<PjitFunction of <function relu at 0x7f986cbc23e0>>),\n",
      "      Linear(\n",
      "        weight=f32[300,500],\n",
      "        bias=f32[300],\n",
      "        in_features=500,\n",
      "        out_features=300,\n",
      "        use_bias=True\n",
      "      ),\n",
      "      Lambda(fn=<PjitFunction of <function relu at 0x7f986cbc23e0>>),\n",
      "      Linear(\n",
      "        weight=f32[8,300],\n",
      "        bias=f32[8],\n",
      "        in_features=300,\n",
      "        out_features=8,\n",
      "        use_bias=True\n",
      "      )\n",
      "    )\n",
      "  ),\n",
      "  mean_fc=Sequential(\n",
      "    layers=(\n",
      "      Linear(\n",
      "        weight=f32[8,8],\n",
      "        bias=f32[8],\n",
      "        in_features=8,\n",
      "        out_features=8,\n",
      "        use_bias=True\n",
      "      ),\n",
      "      Lambda(fn=<PjitFunction of <function relu at 0x7f986cbc23e0>>),\n",
      "      Linear(\n",
      "        weight=f32[8,8],\n",
      "        bias=f32[8],\n",
      "        in_features=8,\n",
      "        out_features=8,\n",
      "        use_bias=True\n",
      "      )\n",
      "    )\n",
      "  ),\n",
      "  log_var_fc=Sequential(\n",
      "    layers=(\n",
      "      Linear(\n",
      "        weight=f32[8,8],\n",
      "        bias=f32[8],\n",
      "        in_features=8,\n",
      "        out_features=8,\n",
      "        use_bias=True\n",
      "      ),\n",
      "      Lambda(fn=<PjitFunction of <function relu at 0x7f986cbc23e0>>),\n",
      "      Linear(\n",
      "        weight=f32[8,8],\n",
      "        bias=f32[8],\n",
      "        in_features=8,\n",
      "        out_features=8,\n",
      "        use_bias=True\n",
      "      )\n",
      "    )\n",
      "  ),\n",
      "  decoder_fcs=Sequential(\n",
      "    layers=(\n",
      "      Linear(\n",
      "        weight=f32[300,8],\n",
      "        bias=f32[300],\n",
      "        in_features=8,\n",
      "        out_features=300,\n",
      "        use_bias=True\n",
      "      ),\n",
      "      Lambda(fn=<PjitFunction of <function relu at 0x7f986cbc23e0>>),\n",
      "      Linear(\n",
      "        weight=f32[500,300],\n",
      "        bias=f32[500],\n",
      "        in_features=300,\n",
      "        out_features=500,\n",
      "        use_bias=True\n",
      "      ),\n",
      "      Lambda(fn=<PjitFunction of <function relu at 0x7f986cbc23e0>>),\n",
      "      Linear(\n",
      "        weight=f32[600,500],\n",
      "        bias=f32[600],\n",
      "        in_features=500,\n",
      "        out_features=600,\n",
      "        use_bias=True\n",
      "      ),\n",
      "      Lambda(fn=<PjitFunction of <function relu at 0x7f986cbc23e0>>),\n",
      "      Linear(\n",
      "        weight=f32[800,600],\n",
      "        bias=f32[800],\n",
      "        in_features=600,\n",
      "        out_features=800,\n",
      "        use_bias=True\n",
      "      ),\n",
      "      Lambda(fn=<PjitFunction of <function relu at 0x7f986cbc23e0>>),\n",
      "      Linear(\n",
      "        weight=f32[1000,800],\n",
      "        bias=f32[1000],\n",
      "        in_features=800,\n",
      "        out_features=1000,\n",
      "        use_bias=True\n",
      "      ),\n",
      "      Lambda(fn=<PjitFunction of <function relu at 0x7f986cbc23e0>>),\n",
      "      Linear(\n",
      "        weight=f32[7781,1000],\n",
      "        bias=f32[7781],\n",
      "        in_features=1000,\n",
      "        out_features=7781,\n",
      "        use_bias=True\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Load the pretrained VAE\n",
    "vae, meta = load_builtin(\"vae\")\n",
    "\n",
    "print(\"VAE Architecture:\")\n",
    "print(f\"  Input channels: {meta['arch']['in_channels']}\")\n",
    "print(f\"  Latent dimension: {meta['arch']['latent_dim']}\")\n",
    "print(f\"\\nModel schema version: {meta['schema']}\")\n",
    "\n",
    "# Load in an uninitialized VAE\n",
    "new_vae = make_SkyVAE(in_channels=7781, latent_dim=8, key=jax.random.PRNGKey(32))\n",
    "print(new_vae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e44d2e47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 9176 sky spectra\n",
      "Wavelength range: 3600.0 - 9824.0 Å\n",
      "Spectrum shape: (9176, 7781)\n",
      "Flux range: -49.059757 - 504.074097\n",
      "Flux mean: 3.130227, std: 6.225298\n",
      "Training set size: 8258\n",
      "Test set size: 918\n",
      "Number of training batches: 65\n",
      "Number of test batches: 8\n"
     ]
    }
   ],
   "source": [
    "# Load the sky spectra value-added catalog\n",
    "vac = SkySpecVAC(version='v1.0', download=True)\n",
    "wavelength, flux, metadata = vac.load()\n",
    "\n",
    "print(f\"Loaded {flux.shape[0]} sky spectra\")\n",
    "print(f\"Wavelength range: {wavelength.min():.1f} - {wavelength.max():.1f} Å\")\n",
    "print(f\"Spectrum shape: {flux.shape}\")\n",
    "print(f\"Flux range: {flux.min():.6f} - {flux.max():.6f}\")\n",
    "print(f\"Flux mean: {flux.mean():.6f}, std: {flux.std():.6f}\")\n",
    "\n",
    "# Create the training/test split\n",
    "dataset_size = len(flux)\n",
    "train_size = int(0.9 * dataset_size)\n",
    "test_size = dataset_size - train_size\n",
    "\n",
    "gen = torch.Generator().manual_seed(32)\n",
    "train_set, test_set = random_split(flux, [train_size, test_size], generator=gen)\n",
    "print(f\"Training set size: {len(train_set)}\")\n",
    "print(f\"Test set size: {len(test_set)}\")\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 128\n",
    "\n",
    "train_loader = NumpyLoader(\n",
    "    dataset=train_set,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_loader = NumpyLoader(\n",
    "    dataset=test_set,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "print(f\"Number of training batches: {len(train_loader)}\")\n",
    "print(f\"Number of test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54d84585",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_base_dir() -> str:\n",
    "    \"\"\"\n",
    "    Determine base directory of the project. This function checks if the\n",
    "    code is running w/in Jupyter notebooks or as standalone scripts.\n",
    "\n",
    "    Returns:\n",
    "        str: Absolute path of the base dir.\n",
    "    \"\"\"\n",
    "    # Check if running in a Jupyter notebook\n",
    "    if 'ipykernel' in sys.modules:\n",
    "        # Assuming notebooks are in the 'notebooks' folder, move one folder up to get base dir.\n",
    "        return os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "    else:\n",
    "        # Get the dir of current script and move one folder up to base dir.\n",
    "        return os.path.abspath(os.path.join(os.path.dirname(__file__), \"..\"))\n",
    "\n",
    "def save(filename: str, project_name: str, hyperparams: 'Any', model: 'Any') -> None:\n",
    "    \"\"\"\n",
    "    Save the model along with hyperparameters to a specified file.\n",
    "\n",
    "    Args:\n",
    "        filename (str): The name of the file to save the model to.\n",
    "        project_name (str): The name of the directory that these saveds models will be stored in.\n",
    "        hyperparams (Dict[str, Any]): Hyperparameters to save along with the model.\n",
    "        model (Any): The model to save.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Fetch the base directory\n",
    "    base_dir = get_base_dir()\n",
    "    \n",
    "    # Determine the full path where models should be saved\n",
    "    save_directory = os.path.join(base_dir, 'saved_models', str(project_name))\n",
    "    \n",
    "    # Ensure the directory exists; if not, create it.\n",
    "    if not os.path.exists(save_directory):\n",
    "        os.makedirs(save_directory)\n",
    "    \n",
    "    # Construct the full path to save the model\n",
    "    full_path = os.path.join(save_directory, filename)\n",
    "\n",
    "    # Save the model and hyperparameters\n",
    "    with open(full_path, \"wb\") as f:\n",
    "        hyperparam_str = json.dumps(hyperparams)\n",
    "        f.write((hyperparam_str + \"\\n\").encode())\n",
    "        eqx.tree_serialise_leaves(f, model)\n",
    "\n",
    "def get_hyperparams_for_model(model):\n",
    "    \"\"\"\n",
    "    Extracts the hyperparameters from the model.\n",
    "\n",
    "    Args:\n",
    "        model (eqx.Module):\n",
    "            Model to extract hyperparameters.\n",
    "    \n",
    "    Returns:\n",
    "        dict: A dictionary of hyperparams.\n",
    "    \"\"\"\n",
    "    # Assume this is the simple VAE\n",
    "    return {\n",
    "        'in_channels': model.in_channels,\n",
    "        'latent_dim': model.latent_dim\n",
    "    }\n",
    "\n",
    "def load_model(filename: str, project_name: str) -> \"Any\":\n",
    "    \"\"\"\n",
    "    Load a model along with its hyperparameters from a specified file.\n",
    "\n",
    "    Args:\n",
    "        filename (str): The name of the file from which to load the model.\n",
    "\n",
    "    Returns:\n",
    "        Any: The deserialized model instance.\n",
    "    \"\"\"\n",
    "    # Fetch base directory path\n",
    "    base_dir = get_base_dir()\n",
    "\n",
    "    # Determine directory where the model is saved\n",
    "    save_directory = os.path.join(base_dir, 'saved_models/' + str(project_name))\n",
    "\n",
    "    # Create the directory if it doesn't exist\n",
    "    if not os.path.exists(save_directory):\n",
    "        os.makedirs(save_directory)\n",
    "\n",
    "    # Construct the full path from where to load the model\n",
    "    full_path = os.path.join(save_directory, filename+\".eqx\")\n",
    "\n",
    "    # Open the file in binary read mode and load the hyperparameters and model\n",
    "    with open(full_path, \"rb\") as f:\n",
    "        hyperparams = json.loads(f.readline().decode())\n",
    "        model = make_SkyVAE(**hyperparams)\n",
    "        return eqx.tree_deserialise_leaves(f, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "297735ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils.py  (or wherever you keep helpers)\n",
    "def default_kernel_sigma(latent_dim: int) -> float:\n",
    "    \"\"\"\n",
    "    Heuristic bandwidth for the RBF kernel, σ = √(2 / d),\n",
    "    recommended by Zhao et al.'s tutorial code.\n",
    "    \"\"\"\n",
    "    return (2.0 / float(latent_dim)) ** 0.5\n",
    "\n",
    "def _rbf_kernel(x, y, *, sigma=1.0):\n",
    "    \"\"\"\n",
    "    Isotropic RBF kernel ‖x-y‖² / (2σ²) – works on batched 2-D arrays.\n",
    "    \"\"\"\n",
    "    x2 = jnp.sum(x * x, axis=1, keepdims=True)\n",
    "    y2 = jnp.sum(y * y, axis=1, keepdims=True)\n",
    "    cross = jnp.dot(x, y.T)\n",
    "    # ‖x‖² + ‖y‖² – 2 x⋅y\n",
    "    dist2 = x2 + y2.T - 2.0 * cross\n",
    "    k = jnp.exp(-dist2 / (2.0 * sigma ** 2))\n",
    "    return k\n",
    "\n",
    "def mmd_rbf_biased(x, y, *, sigma=1.0):\n",
    "    xx = _rbf_kernel(x, x, sigma=sigma).mean()\n",
    "    yy = _rbf_kernel(y, y, sigma=sigma).mean()\n",
    "    xy = _rbf_kernel(x, y, sigma=sigma).mean()\n",
    "    return xx + yy - 2 * xy      #  ≥ 0\n",
    "\n",
    "\n",
    "def loss_fn(model: eqx.Module,\n",
    "            x: jnp.ndarray,\n",
    "            key: jax.random.PRNGKey,\n",
    "            *,\n",
    "            beta: float = 1.0,                 # (1-α) in InfoVAE\n",
    "            lam: float = 10.0,                 # λ in InfoVAE\n",
    "            kernel_sigma: float | str = \"auto\"): # bandwidth or \"auto\")\n",
    "    \"\"\"\n",
    "    InfoVAE-MMD objective:\n",
    "        L = recon_loss  +  beta * KL[q(z|x) || p(z)]\n",
    "                         +  lam  * MMD[q(z) , p(z)]\n",
    "    \"\"\"\n",
    "\n",
    "    # ── 0. choose σ heuristically if requested ────────────────────────\n",
    "    if kernel_sigma is None or kernel_sigma == \"auto\":\n",
    "        kernel_sigma = default_kernel_sigma(model.latent_dim)\n",
    "\n",
    "    # ── 1. forward pass ───────────────────────────────────────────────\n",
    "    # x.shape = (B, L) where B=batch_size, L=7781\n",
    "    \n",
    "    ## Encode the input\n",
    "    mean, logvar = jax.vmap(model.encode)(x)  # (B, latent_dim)\n",
    "    \n",
    "    ## Sample from the latent space\n",
    "    # FIX: Need to vmap sample() to give each sample its own random key\n",
    "    batch_size = x.shape[0]\n",
    "    keys = jax.random.split(key, batch_size)  # Create batch_size unique keys\n",
    "    z = jax.vmap(model.sample)(mean, logvar, keys)  # Vmap over all 3 args\n",
    "    \n",
    "    ## Decode the latent representation\n",
    "    out_recon = jax.vmap(model.decode)(z)\n",
    "\n",
    "    # reconstruction (L2/MSE)\n",
    "    recon_loss = jnp.mean(optax.l2_loss(out_recon, x))\n",
    "\n",
    "    # KL[q(z|x) ‖ p(z)] per-sample, then mean over batch\n",
    "    kl_loss = -0.5 * jnp.mean(1 + logvar - jnp.square(mean) - jnp.exp(logvar))\n",
    "\n",
    "    # MMD between aggregated posterior q(z) and prior p(z) = N(0,I)\n",
    "    z_prior = jax.random.normal(key, shape=z.shape)     # same shape\n",
    "    mmd_loss = mmd_rbf_biased(z, z_prior, sigma=kernel_sigma)\n",
    "\n",
    "    total = recon_loss + beta * kl_loss + (lam - beta) * mmd_loss\n",
    "    aux = dict(\n",
    "        recon = recon_loss,\n",
    "        kl_weighted = beta * kl_loss,\n",
    "        mmd_weighted = (lam - beta) * mmd_loss,\n",
    "        loss_z = beta * kl_loss + (lam - beta) * mmd_loss\n",
    "    )\n",
    "\n",
    "    return total, aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35eecb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,\n",
    "          train_loader,\n",
    "          test_loader,\n",
    "          optim,\n",
    "          *,\n",
    "          n_epochs: int,\n",
    "          beta: float,\n",
    "          lam: float,\n",
    "          kernel_sigma: float | str):\n",
    "    \n",
    "    # Train the arrays in our model, filtering out everything else (e.g. activations)\n",
    "    opt_state = optim.init(eqx.filter(model, eqx.is_array))\n",
    "\n",
    "    # Initialize a variable to track the best train loss\n",
    "    best_train_loss = float('inf')\n",
    "\n",
    "    # Always wrap everything: computing gradients, running optimizer, and \n",
    "    # updating the model into a single JIT region. This ensures things run\n",
    "    # as fast as possible.\n",
    "    @eqx.filter_jit\n",
    "    def make_step(model, x, opt_state, beta, lam, kernel_sigma, key):\n",
    "        # Compute the gradients\n",
    "        (loss, aux), grads = eqx.filter_value_and_grad(loss_fn, has_aux=True)(\n",
    "                                                         model,\n",
    "                                                         x=x,\n",
    "                                                         key=key, \n",
    "                                                         beta=beta,\n",
    "                                                         lam=lam,\n",
    "                                                         kernel_sigma=kernel_sigma)\n",
    "        updates, opt_state = optim.update(grads, opt_state, model)\n",
    "        model = eqx.apply_updates(model, updates)\n",
    "        return model, opt_state, loss, aux\n",
    "    \n",
    "    key_seed = 0\n",
    "    train_losses = []\n",
    "    # Loop over every epoch\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        # Total, recon, weighted KL, weighted MMD, n samples\n",
    "        epoch_train_loss, rec, klw, mmdw, loss_z, batch_count = 0.0, 0.0, 0.0, 0.0, 0.0, 0\n",
    "\n",
    "        for step, (x) in enumerate(train_loader):\n",
    "            # Update keys for every batch, so that we are\n",
    "            # getting different noise values.\n",
    "            key_seed += 1  # <- FIX: Increment the key seed!\n",
    "            # Loss computation & parameter update\n",
    "            model, opt_state, train_loss, aux = make_step(model=model,\n",
    "                                                     x=x,  # <- FIX: Use x directly\n",
    "                                                     key=jax.random.PRNGKey(key_seed),\n",
    "                                                     beta=beta,\n",
    "                                                     lam=lam,\n",
    "                                                     kernel_sigma=kernel_sigma,\n",
    "                                                     opt_state=opt_state)\n",
    "            epoch_train_loss += float(train_loss)\n",
    "            batch_count += 1\n",
    "\n",
    "        epoch_train_loss /= batch_count            \n",
    "            # # Convert to Python floats and accumulate\n",
    "            # loss_f = float(train_loss)\n",
    "            # recon_f = float(aux[\"recon\"])\n",
    "            # kl_f = float(aux[\"kl_weighted\"])\n",
    "            # mmd_f = float(aux[\"mmd_weighted\"])\n",
    "            # loss_z_f = float(aux[\"loss_z\"])\n",
    "            \n",
    "        # epoch-averaged losses\n",
    "        train_losses.append(epoch_train_loss)\n",
    "        # Check if current loss is best loss\n",
    "        if epoch_train_loss < best_train_loss:\n",
    "            # Update the best training loss\n",
    "            best_train_loss = epoch_train_loss\n",
    "            # Save the model at the end of each epoch\n",
    "            hyperparams = get_hyperparams_for_model(model)\n",
    "            # save(\n",
    "            #     filename=\"vae1.eqx\",\n",
    "            #     project_name=str(project_name),\n",
    "            #     hyperparams=hyperparams,\n",
    "            #     model=model\n",
    "            # )\n",
    "\n",
    "        if epoch % 1 == 0:   \n",
    "            print(f\"epoch {epoch}, train loss: {epoch_train_loss:.8f}\")#, recon: {rec:.8f}, kl: {klw:.8f}, mmd: {mmdw:.8f}\")\n",
    "\n",
    "    return model, train_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6ab58c",
   "metadata": {},
   "outputs": [],
   "source": "model = make_SkyVAE(in_channels=7781, latent_dim=8, key=jax.random.PRNGKey(32))\n\nvac = SkySpecVAC(version='v1.0', download=True)\nwavelength, flux, metadata = vac.load()\n\n# Create the training/test split\ndataset_size = len(flux)\ntrain_size = int(0.9 * dataset_size)\ntest_size = dataset_size - train_size\n\ngen = torch.Generator().manual_seed(32)\ntrain_set, test_set = random_split(flux, [train_size, test_size], generator=gen)\n\n# Create data loaders\nbatch_size = 128\n\ntrain_loader = NumpyLoader(\n    dataset=train_set,\n    batch_size=batch_size,\n    shuffle=True\n)\n\ntest_loader = NumpyLoader(\n    dataset=test_set,\n    batch_size=batch_size,\n    shuffle=False\n)\n\n# FIX: Add gradient clipping to prevent NaN explosions\noptim = optax.chain(\n    optax.clip_by_global_norm(1.0),  # Clip gradients with norm > 1.0\n    optax.adam(learning_rate=1e-4)\n)\n\nkernel_sigma = default_kernel_sigma(model.latent_dim)"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa8c4e28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DEBUG: Testing forward pass ===\n",
      "Batch shape: (128, 7781)\n",
      "Batch min: -0.2016, max: 386.4428, mean: 2.8165\n",
      "\n",
      "1. Testing encode...\n",
      "   Mean shape: (128, 8), range: [-0.2774, 0.2027]\n",
      "   Logvar shape: (128, 8), range: [-0.2045, 0.3213]\n",
      "\n",
      "2. Testing sample...\n",
      "   Latent shape: (128, 8), range: [-3.5655, 3.1428]\n",
      "\n",
      "3. Testing decode...\n",
      "   Recon shape: (128, 7781), range: [-0.0755, 0.0758]\n",
      "\n",
      "4. Testing loss computation...\n",
      "   Total loss: 22.4201602935791\n",
      "   Recon: 22.40451431274414\n",
      "   KL weighted: 2.695000330277253e-05\n",
      "   MMD weighted: 0.015619147568941116\n",
      "\n",
      "=== DEBUG COMPLETE ===\n"
     ]
    }
   ],
   "source": [
    "# DEBUG: Test forward pass and loss computation\n",
    "print(\"=== DEBUG: Testing forward pass ===\")\n",
    "\n",
    "# Get a single batch\n",
    "test_batch = next(iter(train_loader))\n",
    "print(f\"Batch shape: {test_batch.shape}\")\n",
    "print(f\"Batch min: {test_batch.min():.4f}, max: {test_batch.max():.4f}, mean: {test_batch.mean():.4f}\")\n",
    "\n",
    "# Test encode\n",
    "print(\"\\n1. Testing encode...\")\n",
    "mean, logvar = jax.vmap(model.encode)(test_batch)\n",
    "print(f\"   Mean shape: {mean.shape}, range: [{mean.min():.4f}, {mean.max():.4f}]\")\n",
    "print(f\"   Logvar shape: {logvar.shape}, range: [{logvar.min():.4f}, {logvar.max():.4f}]\")\n",
    "\n",
    "# Test sample (with proper batching)\n",
    "print(\"\\n2. Testing sample...\")\n",
    "batch_size = test_batch.shape[0]\n",
    "keys = jax.random.split(jax.random.PRNGKey(0), batch_size)\n",
    "z = jax.vmap(model.sample)(mean, logvar, keys)\n",
    "print(f\"   Latent shape: {z.shape}, range: [{z.min():.4f}, {z.max():.4f}]\")\n",
    "\n",
    "# Test decode\n",
    "print(\"\\n3. Testing decode...\")\n",
    "out_recon = jax.vmap(model.decode)(z)\n",
    "print(f\"   Recon shape: {out_recon.shape}, range: [{out_recon.min():.4f}, {out_recon.max():.4f}]\")\n",
    "\n",
    "# Test loss\n",
    "print(\"\\n4. Testing loss computation...\")\n",
    "loss_val, aux = loss_fn(model, test_batch, jax.random.PRNGKey(0), beta=1e-3, lam=1, kernel_sigma=kernel_sigma)\n",
    "print(f\"   Total loss: {loss_val}\")\n",
    "print(f\"   Recon: {aux['recon']}\")\n",
    "print(f\"   KL weighted: {aux['kl_weighted']}\")\n",
    "print(f\"   MMD weighted: {aux['mmd_weighted']}\")\n",
    "\n",
    "print(\"\\n=== DEBUG COMPLETE ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad462651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, train loss: nan\n",
      "epoch 2, train loss: nan\n",
      "epoch 3, train loss: nan\n",
      "epoch 4, train loss: nan\n",
      "epoch 5, train loss: nan\n",
      "epoch 6, train loss: nan\n",
      "epoch 7, train loss: nan\n",
      "epoch 8, train loss: nan\n",
      "epoch 9, train loss: nan\n",
      "epoch 10, train loss: nan\n",
      "epoch 11, train loss: nan\n",
      "epoch 12, train loss: nan\n",
      "epoch 13, train loss: nan\n",
      "epoch 14, train loss: nan\n",
      "epoch 15, train loss: nan\n"
     ]
    }
   ],
   "source": [
    "trained_model, train_loss = train(model=model,\n",
    "          train_loader=train_loader,\n",
    "          test_loader=test_loader,\n",
    "          optim=optim,\n",
    "          n_epochs=15,\n",
    "          beta=1e-3,\n",
    "          lam=4,\n",
    "          kernel_sigma=kernel_sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "80b8561b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (30,) and (15,)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mplt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m30\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loss\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/desisky_dev/lib/python3.11/site-packages/matplotlib/pyplot.py:3838\u001b[39m, in \u001b[36mplot\u001b[39m\u001b[34m(scalex, scaley, data, *args, **kwargs)\u001b[39m\n\u001b[32m   3830\u001b[39m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Axes.plot)\n\u001b[32m   3831\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mplot\u001b[39m(\n\u001b[32m   3832\u001b[39m     *args: \u001b[38;5;28mfloat\u001b[39m | ArrayLike | \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3836\u001b[39m     **kwargs,\n\u001b[32m   3837\u001b[39m ) -> \u001b[38;5;28mlist\u001b[39m[Line2D]:\n\u001b[32m-> \u001b[39m\u001b[32m3838\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgca\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3839\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3840\u001b[39m \u001b[43m        \u001b[49m\u001b[43mscalex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscalex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3841\u001b[39m \u001b[43m        \u001b[49m\u001b[43mscaley\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscaley\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3842\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3843\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3844\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/desisky_dev/lib/python3.11/site-packages/matplotlib/axes/_axes.py:1777\u001b[39m, in \u001b[36mAxes.plot\u001b[39m\u001b[34m(self, scalex, scaley, data, *args, **kwargs)\u001b[39m\n\u001b[32m   1534\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1535\u001b[39m \u001b[33;03mPlot y versus x as lines and/or markers.\u001b[39;00m\n\u001b[32m   1536\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1774\u001b[39m \u001b[33;03m(``'green'``) or hex strings (``'#008000'``).\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1776\u001b[39m kwargs = cbook.normalize_kwargs(kwargs, mlines.Line2D)\n\u001b[32m-> \u001b[39m\u001b[32m1777\u001b[39m lines = [*\u001b[38;5;28mself\u001b[39m._get_lines(\u001b[38;5;28mself\u001b[39m, *args, data=data, **kwargs)]\n\u001b[32m   1778\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m lines:\n\u001b[32m   1779\u001b[39m     \u001b[38;5;28mself\u001b[39m.add_line(line)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/desisky_dev/lib/python3.11/site-packages/matplotlib/axes/_base.py:297\u001b[39m, in \u001b[36m_process_plot_var_args.__call__\u001b[39m\u001b[34m(self, axes, data, return_kwargs, *args, **kwargs)\u001b[39m\n\u001b[32m    295\u001b[39m     this += args[\u001b[32m0\u001b[39m],\n\u001b[32m    296\u001b[39m     args = args[\u001b[32m1\u001b[39m:]\n\u001b[32m--> \u001b[39m\u001b[32m297\u001b[39m \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_plot_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    298\u001b[39m \u001b[43m    \u001b[49m\u001b[43maxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[43m=\u001b[49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    299\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_kwargs\u001b[49m\n\u001b[32m    300\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/desisky_dev/lib/python3.11/site-packages/matplotlib/axes/_base.py:494\u001b[39m, in \u001b[36m_process_plot_var_args._plot_args\u001b[39m\u001b[34m(self, axes, tup, kwargs, return_kwargs, ambiguous_fmt_datakey)\u001b[39m\n\u001b[32m    491\u001b[39m     axes.yaxis.update_units(y)\n\u001b[32m    493\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m x.shape[\u001b[32m0\u001b[39m] != y.shape[\u001b[32m0\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m494\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mx and y must have same first dimension, but \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    495\u001b[39m                      \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mhave shapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    496\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m x.ndim > \u001b[32m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m y.ndim > \u001b[32m2\u001b[39m:\n\u001b[32m    497\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mx and y can be no greater than 2D, but have \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    498\u001b[39m                      \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mshapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: x and y must have same first dimension, but have shapes (30,) and (15,)"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.arange(30), train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018ff109",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "desisky_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}